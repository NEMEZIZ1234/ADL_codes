SONARCUBE
#Configure here general information about the environment, such as SonarQube server connection 
details for example
#No information about specific project should appear here
#----- Default SonarQube server
#sonar.host.url=http://localhost:9000
#----- Default source code encoding
#sonar.sourceEncoding=UTF-8
sonar.projectKey=SFIT_SAST3
sonar.projectName=SFIT_SAST3
sonar.projectVersion=1.0
#sonar.projectBaseDir=C:\SFITJobs
sonar.sources=C:\SFITJobs\PY Scripts

Q1).To understand and implement the concept of a pipeline by using GitHub as a platform for continuous integration and deployment on a sample application.
Link: https://github.com/imoisharma/aws-codepipeline-s3-codedeploy-linux-2.0

.gitHub/workflow/ci-cd-pipeline.yml
name: CI Pipeline
on:
 push:
  branches: [ "main" ]
 pull_request:
  branches: [ "main" ]
jobs:
 build:
  runs-on: ubuntu-latest
  steps:
  - name: Checkout code
    uses: actions/checkout@v3
  - name: Set up Node.js
    uses: actions/setup-node@v3
    with:
     node-version: '16'
  - name: Install dependencies
    run: npm install
  - name: Run tests
    run: npm test



Q2).To install terraform on a Windows/Linux machine and build, apply, and destroy EC2 instance over AWS using Terraform.

Code:

provider "aws" {
  access_key = ""
  secret_key = ""
  region     = "us-east-1"
}

# Launch EC2 instances
resource "aws_instance" "terraformUser" {
  ami           = ""  # Replace with a region-specific valid AMI
  instance_type = "t3.micro"
  count         = 3

  tags = {
    Name = "praj"
  }
}

Q3).To install Kubectl and execute kubectl commands to manage clusters and deploy your first Kubernetes application (Nginx)
commands
docker login
docker pull registry.k8s.io/kube-apiserver:v1.32.2
docker pull registry.k8s.io/pause:3.9
docker pull registry.k8s.io/coredns/coredns:v1.11.1
docker pull registry.k8s.io/kube-controller-manager:v1.32.2
docker pull registry.k8s.io/kube-scheduler:v1.32.2
docker pull registry.k8s.io/kube-proxy:v1.32.2
docker pull registry.k8s.io/etcd:3.5.15-0

Install and verify kubectl
$version = Invoke-RestMethod -Uri "https://dl.k8s.io/release/stable.txt"

       Invoke-WebRequest -Uri "https://dl.k8s.io/release/$version/bin/windows/amd64/kubectl.exe" -OutFile "$env:USERPROFILE\Downloads\kubectl.exe"

Move-Item "$env:USERPROFILE\Downloads\kubectl.exe" "C:\Windows\System32\kubectl.exe

kubectl version –client 

MAIN STEPS:
kubectl cluster-info

kubectl get nodes

kubectl create deployment my-nginx --image=nginx

kubectl get deployments

 kubectl get pods

 kubectl expose deployment my-nginx --type=NodePort --port=80

kubectl get svc

kubectl get all

kubectl delete svc my-nginx

kubectl delete deployment my-nginx

kubectl get all


Q4).To install Kubectl and execute kubectl commands to manage clusters and deploy your first Kubernetes application (Apache)

Commands:
docker login
docker pull registry.k8s.io/kube-apiserver:v1.32.2
docker pull registry.k8s.io/pause:3.9
docker pull registry.k8s.io/coredns/coredns:v1.11.1
docker pull registry.k8s.io/kube-controller-manager:v1.32.2
docker pull registry.k8s.io/kube-scheduler:v1.32.2
docker pull registry.k8s.io/kube-proxy:v1.32.2
docker pull registry.k8s.io/etcd:3.5.15-0

Install and verify kubectl
$version = Invoke-RestMethod -Uri "https://dl.k8s.io/release/stable.txt"

       Invoke-WebRequest -Uri "https://dl.k8s.io/release/$version/bin/windows/amd64/kubectl.exe" -OutFile "$env:USERPROFILE\Downloads\kubectl.exe"

Move-Item "$env:USERPROFILE\Downloads\kubectl.exe" "C:\Windows\System32\kubectl.exe

kubectl version –client 

MAIN STEPS:
kubectl cluster-info

kubectl get nodes

kubectl create deployment my-apache --image=httpd

kubectl expose deployment my-apache --type=NodePort --port=80

kubectl get svc my-apache

kubectl get all

kubectl delete svc my-apache

kubectl delete deployment my-apache

kubectl get all


Q5).To deploy Docker and build, apply, and destroy  docker image using Terraform.

terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "2.14.0"
    }
  }
}

provider "docker" {
  host = "npipe:////.//pipe//docker_engine"  # This is for Windows Docker named pipe
}

resource "docker_image" "redis" {
  name = "redis:latest"
}

Q6).To deploy and manage an NGINX web server container using Terraform with Docker as the provider.


terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 3.0.2"
    }
  }
}

provider "docker" {}

resource "docker_image" "nginx_image" {
  name         = "nginx:latest"
  keep_locally = false
}

resource "docker_container" "nginx_container" {
  name  = "terraform-nginx"
  image = docker_image.nginx_image.name

  ports {
    internal = 80
    external = 8081
  }
}

Or
terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 3.0.2"
    }
  }
}

provider "docker" {}

resource "docker_image" "nginx_image" {
  name         = "nginx:latest"
  keep_locally = true
}

resource "docker_container" "nginx_container" {
  name  = "terraform-nginx-custom"
  image = docker_image.nginx_image.name

  ports {
    internal = 80
    external = 8082
  }

  volumes {
    host_path      = "c:/adl7c/pra.html"
    container_path = "/usr/share/nginx/html/pra.html"
  }
}

Q11)To create an AWS Lambda function to log “an object has been added” when adding the object to the s3 bucket.


import json
import boto3

s3 = boto3.client('s3')  # Create an S3 client

def lambda_handler(event, context):
    bucket = 'prajyotibucket1234'  # Define your S3 bucket name

    dataToUpload = {}  # Initialize an empty dictionary

    # Add key-value pairs to the dictionary
    dataToUpload['PID'] = '1400'
    dataToUpload['DEPT'] = 'INFT'
    dataToUpload['NAME'] = 'PRAJYOTI'
    dataToUpload['FILE'] = 'ABC 1111'

    # Define the filename, adding the .json extension
    fileName = 'ABC 1111' + '.json'

    # Convert the dataToUpload dictionary to a JSON byte stream
    uploadByteStream = bytes(json.dumps(dataToUpload).encode('UTF-8'))

    # Upload the JSON byte stream to S3
    s3.put_object(Bucket=bucket, Key=fileName, Body=uploadByteStream)

    print(f"Object has been uploaded to {bucket} with key {fileName}")



